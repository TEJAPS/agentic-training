{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c730f5",
   "metadata": {},
   "source": [
    "# Subword Tokenization Demo: BPE & WordPiece (from scratch)\n",
    "\n",
    "This notebook demonstrates **Byte Pair Encoding (BPE)** and a simplified **WordPiece** tokenization flow from scratch.\n",
    "\n",
    "These are educational implementations for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b60f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utilities ---\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    # Lowercase and split on non-alphanumerics (keep apostrophes)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9']+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf4aa7",
   "metadata": {},
   "source": [
    "## Part 1 — Byte Pair Encoding (BPE)\n",
    "Start from characters; repeatedly merge the most frequent adjacent pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c432f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BPE Implementation ---\n",
    "def get_pair_frequencies(tokenized_words):\n",
    "    pairs = Counter()\n",
    "    for symbols in tokenized_words:\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(tokenized_words, pair):\n",
    "    a, b = pair\n",
    "    merged_token = a + '·' + b  # visual joiner\n",
    "    new_words = []\n",
    "    for symbols in tokenized_words:\n",
    "        i = 0\n",
    "        new_symbols = []\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                new_symbols.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_words.append(new_symbols)\n",
    "    return new_words\n",
    "\n",
    "def learn_bpe(corpus, target_vocab_size=60, verbose=True):\n",
    "    words = []\n",
    "    for line in corpus:\n",
    "        words.extend(simple_preprocess(line))\n",
    "    tokenized_words = [list(w) + ['</w>'] for w in words]\n",
    "    vocab = set(sym for w in tokenized_words for sym in w)\n",
    "    merges = []\n",
    "    if verbose:\n",
    "        print(f'Initial vocab size: {len(vocab)}')\n",
    "        print('Sample word:', tokenized_words[0])\n",
    "    while len(vocab) < target_vocab_size:\n",
    "        pair_freq = get_pair_frequencies(tokenized_words)\n",
    "        if not pair_freq:\n",
    "            break\n",
    "        best_pair, best_freq = pair_freq.most_common(1)[0]\n",
    "        tokenized_words = merge_pair(tokenized_words, best_pair)\n",
    "        merged_token = best_pair[0] + '·' + best_pair[1]\n",
    "        merges.append(best_pair)\n",
    "        vocab.add(merged_token)\n",
    "        if verbose:\n",
    "            print(f\"Merge {len(merges):>3}: {best_pair} -> '{merged_token}' (freq={best_freq}) | vocab={len(vocab)}\")\n",
    "    return merges, vocab, tokenized_words\n",
    "\n",
    "def bpe_tokenize(word, merges):\n",
    "    symbols = list(word) + ['</w>']\n",
    "    for a, b in merges:\n",
    "        i = 0\n",
    "        merged = []\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                merged.append(a + '·' + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                merged.append(symbols[i])\n",
    "                i += 1\n",
    "        symbols = merged\n",
    "    return symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fea8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 20\n",
      "Sample word: ['p', 'i', 'z', 'z', 'a', '</w>']\n",
      "Merge   1: ('p', 'i') -> 'p·i' (freq=8) | vocab=21\n",
      "Merge   2: ('z', 'z') -> 'z·z' (freq=8) | vocab=22\n",
      "Merge   3: ('p·i', 'z·z') -> 'p·i·z·z' (freq=7) | vocab=23\n",
      "Merge   4: ('p·i·z·z', 'a') -> 'p·i·z·z·a' (freq=6) | vocab=24\n",
      "Merge   5: ('s', '</w>') -> 's·</w>' (freq=6) | vocab=25\n",
      "Merge   6: ('e', '</w>') -> 'e·</w>' (freq=5) | vocab=26\n",
      "Merge   7: ('p·i·z·z·a', '</w>') -> 'p·i·z·z·a·</w>' (freq=4) | vocab=27\n",
      "Merge   8: ('i', 's·</w>') -> 'i·s·</w>' (freq=3) | vocab=28\n",
      "Merge   9: ('a', 's') -> 'a·s' (freq=2) | vocab=29\n",
      "Merge  10: ('y', '</w>') -> 'y·</w>' (freq=2) | vocab=30\n",
      "Merge  11: ('f', 'l') -> 'f·l' (freq=2) | vocab=31\n",
      "Merge  12: ('l', 'e·</w>') -> 'l·e·</w>' (freq=2) | vocab=32\n",
      "Merge  13: ('v', 'e·</w>') -> 'v·e·</w>' (freq=2) | vocab=33\n",
      "Merge  14: ('e', 'a') -> 'e·a' (freq=2) | vocab=34\n",
      "Merge  15: ('e', 'r') -> 'e·r' (freq=2) | vocab=35\n",
      "Merge  16: ('t', 'a·s') -> 't·a·s' (freq=1) | vocab=36\n",
      "Merge  17: ('t·a·s', 't') -> 't·a·s·t' (freq=1) | vocab=37\n",
      "Merge  18: ('t·a·s·t', 'y·</w>') -> 't·a·s·t·y·</w>' (freq=1) | vocab=38\n",
      "Merge  19: ('p·i·z·z·a', 'z·z') -> 'p·i·z·z·a·z·z' (freq=1) | vocab=39\n",
      "Merge  20: ('p·i·z·z·a·z·z', '</w>') -> 'p·i·z·z·a·z·z·</w>' (freq=1) | vocab=40\n",
      "Merge  21: ('f·l', 'a·s') -> 'f·l·a·s' (freq=1) | vocab=41\n",
      "Merge  22: ('f·l·a·s', 'h') -> 'f·l·a·s·h' (freq=1) | vocab=42\n",
      "Merge  23: ('f·l·a·s·h', 'y·</w>') -> 'f·l·a·s·h·y·</w>' (freq=1) | vocab=43\n",
      "Merge  24: ('u', 'n') -> 'u·n' (freq=1) | vocab=44\n",
      "Merge  25: ('u·n', 'b') -> 'u·n·b' (freq=1) | vocab=45\n",
      "Merge  26: ('u·n·b', 'e') -> 'u·n·b·e' (freq=1) | vocab=46\n",
      "Merge  27: ('u·n·b·e', 'l') -> 'u·n·b·e·l' (freq=1) | vocab=47\n",
      "Merge  28: ('u·n·b·e·l', 'i') -> 'u·n·b·e·l·i' (freq=1) | vocab=48\n",
      "Merge  29: ('u·n·b·e·l·i', 'e') -> 'u·n·b·e·l·i·e' (freq=1) | vocab=49\n",
      "Merge  30: ('u·n·b·e·l·i·e', 'v') -> 'u·n·b·e·l·i·e·v' (freq=1) | vocab=50\n",
      "Merge  31: ('u·n·b·e·l·i·e·v', 'a') -> 'u·n·b·e·l·i·e·v·a' (freq=1) | vocab=51\n",
      "Merge  32: ('u·n·b·e·l·i·e·v·a', 'b') -> 'u·n·b·e·l·i·e·v·a·b' (freq=1) | vocab=52\n",
      "Merge  33: ('u·n·b·e·l·i·e·v·a·b', 'l·e·</w>') -> 'u·n·b·e·l·i·e·v·a·b·l·e·</w>' (freq=1) | vocab=53\n",
      "Merge  34: ('f·l', 'a') -> 'f·l·a' (freq=1) | vocab=54\n",
      "Merge  35: ('f·l·a', 'v') -> 'f·l·a·v' (freq=1) | vocab=55\n",
      "Merge  36: ('f·l·a·v', 'o') -> 'f·l·a·v·o' (freq=1) | vocab=56\n",
      "Merge  37: ('f·l·a·v·o', 'r') -> 'f·l·a·v·o·r' (freq=1) | vocab=57\n",
      "Merge  38: ('f·l·a·v·o·r', 's·</w>') -> 'f·l·a·v·o·r·s·</w>' (freq=1) | vocab=58\n",
      "Merge  39: ('o', 'f') -> 'o·f' (freq=1) | vocab=59\n",
      "Merge  40: ('o·f', '</w>') -> 'o·f·</w>' (freq=1) | vocab=60\n",
      "\n",
      "First 15 merges:\n",
      " 1. ('p', 'i')\n",
      " 2. ('z', 'z')\n",
      " 3. ('p·i', 'z·z')\n",
      " 4. ('p·i·z·z', 'a')\n",
      " 5. ('s', '</w>')\n",
      " 6. ('e', '</w>')\n",
      " 7. ('p·i·z·z·a', '</w>')\n",
      " 8. ('i', 's·</w>')\n",
      " 9. ('a', 's')\n",
      "10. ('y', '</w>')\n",
      "11. ('f', 'l')\n",
      "12. ('l', 'e·</w>')\n",
      "13. ('v', 'e·</w>')\n",
      "14. ('e', 'a')\n",
      "15. ('e', 'r')\n",
      "\n",
      "BPE tokenize examples:\n",
      "pizza -> ['p·i·z·z·a·</w>']\n",
      "pizzazz -> ['p·i·z·z·a·z·z·</w>']\n",
      "pineapple -> ['p·i', 'n', 'e·a', 'p', 'p', 'l·e·</w>']\n",
      "unbelievable -> ['u·n·b·e·l·i·e·v·a·b·l·e·</w>']\n"
     ]
    }
   ],
   "source": [
    "# --- BPE Demo ---\n",
    "corpus = [\n",
    "    'pizza is tasty',\n",
    "    'pizzazz is flashy',\n",
    "    'unbelievable flavors of pizzas',\n",
    "    'i love pineapple pizza',\n",
    "    'cheese on pizza is great',\n",
    "    'pizzerias serve pizza',\n",
    "]\n",
    "merges, vocab, tokenized_words = learn_bpe(corpus, target_vocab_size=60, verbose=True)\n",
    "print('\\nFirst 15 merges:')\n",
    "for i, m in enumerate(merges[:15], 1):\n",
    "    print(f'{i:>2}. {m}')\n",
    "print('\\nBPE tokenize examples:')\n",
    "for w in ['pizza', 'pizzazz', 'pineapple', 'unbelievable']:\n",
    "    print(w, '->', bpe_tokenize(w, merges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251df529",
   "metadata": {},
   "source": [
    "## Part 2 — WordPiece (Simplified)\n",
    "We'll build a toy vocab by substring frequency and tokenize with greedy longest-match (MaxMatch). We use the '##' prefix for continuation pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e61d604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WordPiece (Simplified) ---\n",
    "from collections import defaultdict\n",
    "\n",
    "def all_substrings(word, max_len=8):\n",
    "    subs = []\n",
    "    n = len(word)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, min(n, i+max_len) + 1):\n",
    "            subs.append(word[i:j])\n",
    "    return subs\n",
    "\n",
    "def build_wordpiece_vocab(corpus, target_vocab_size=120, max_sub_len=8, verbose=True):\n",
    "    words = []\n",
    "    for line in corpus:\n",
    "        words.extend(simple_preprocess(line))\n",
    "    sub_freq = Counter()\n",
    "    char_set = set()\n",
    "    for w in words:\n",
    "        char_set.update(w)\n",
    "        for s in all_substrings(w, max_len=max_sub_len):\n",
    "            sub_freq[s] += 1\n",
    "    vocab = set(list(char_set))\n",
    "    specials = {'[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'}\n",
    "    vocab |= specials\n",
    "    ranked = sorted(sub_freq.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True)\n",
    "    if verbose:\n",
    "        print(f'Initial vocab size (chars+specials): {len(vocab)}')\n",
    "        print('Top 10 substrings by freq:', ranked[:10])\n",
    "    for s, freq in ranked:\n",
    "        if len(vocab) >= target_vocab_size:\n",
    "            break\n",
    "        if s not in vocab:\n",
    "            vocab.add(s)\n",
    "        if len(vocab) < target_vocab_size and len(s) > 1:\n",
    "            vocab.add('##' + s)\n",
    "    return vocab\n",
    "\n",
    "def wordpiece_tokenize(word, vocab, unk_token='[UNK]'):\n",
    "    tokens = []\n",
    "    start = 0\n",
    "    while start < len(word):\n",
    "        end = len(word)\n",
    "        cur_sub = None\n",
    "        while start < end:\n",
    "            piece = word[start:end] if len(tokens) == 0 else '##' + word[start:end]\n",
    "            if piece in vocab:\n",
    "                cur_sub = piece\n",
    "                break\n",
    "            end -= 1\n",
    "        if cur_sub is None:\n",
    "            return [unk_token]\n",
    "        tokens.append(cur_sub)\n",
    "        advance = len(cur_sub) if len(tokens) == 1 else len(cur_sub) - 2\n",
    "        start += advance\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be15c2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size (chars+specials): 24\n",
      "Top 10 substrings by freq: [('z', 16), ('i', 14), ('a', 13), ('e', 13), ('p', 10), ('s', 10), ('pi', 8), ('zz', 8), ('pizz', 7), ('piz', 7)]\n",
      "        pizza -> ['pizza']\n",
      "      pizzazz -> ['pizzazz']\n",
      "    pineapple -> ['[UNK]']\n",
      " unbelievable -> ['[UNK]']\n",
      "    pizzerias -> ['[UNK]']\n",
      "   cheesiness -> ['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "# --- WordPiece Demo ---\n",
    "vocab_wp = build_wordpiece_vocab(corpus, target_vocab_size=120, max_sub_len=8, verbose=True)\n",
    "examples = ['pizza', 'pizzazz', 'pineapple', 'unbelievable', 'pizzerias', 'cheesiness']\n",
    "for w in examples:\n",
    "    print(f\"{w:>13} -> {wordpiece_tokenize(w, vocab_wp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41294ef-c39c-4a99-835b-863b5d3654ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aad750-4eef-484a-88f0-33ebdf4a7918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
